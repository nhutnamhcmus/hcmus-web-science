{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab03_WebCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaqjihtwRrys"
      },
      "source": [
        "# Lab03: Web Crawler (Continue) & Information Retrieval.\n",
        "\n",
        "- MSSV: 18120061\n",
        "- Họ và tên: Lê Nhựt Nam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUG7FFTsRryt"
      },
      "source": [
        "## Yêu cầu bài tập\n",
        "\n",
        "**Cách làm bài**\n",
        "\n",
        "\n",
        "Bạn sẽ làm trực tiếp trên file notebook này; từ `TODO` cho biết những phần mà bạn cần phải làm.\n",
        "\n",
        "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
        "\n",
        "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
        "\n",
        "**Cách nộp bài**\n",
        "\n",
        "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
        "\n",
        "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file notebook, file `t_data.txt` và file `raw_data` của các bạn (nếu file này kích thước lớn các bạn có thể chép link vào `link_data.txt`), nén thư mục `MSSV` này lại và nộp trên moodle.\n",
        "\n",
        "**Nội dung bài tập**\n",
        "\n",
        "Cài đặt một web crawler để thu thập dữ liệu từ: https://en.wikipedia.org/wiki/Web_mining."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HlhdT6BRryu"
      },
      "source": [
        "## Nội dung bài tập"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9-ZyiLjRryv"
      },
      "source": [
        "Cài đặt một Web crawler đơn giản bắt đầu từ URL: https://en.wikipedia.org/wiki/Web_mining, tìm liên kết và thu thập dữ liệu trong HTML tại URL này sau đó lặp lại với các URL vừa tìm được.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJktAwbCOyod"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import string\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMSlOpSsRryv"
      },
      "source": [
        "## 1. Thu thập đường dẫn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZZ9lSUPRryw"
      },
      "source": [
        "- Robot.txt:  https://en.wikipedia.org/robots.txt\n",
        "- **Bước 1**: Thu thập đường dẫn từ https://en.wikipedia.org/wiki/Web_mining. Lưu trữ vào một danh sách `url_list`. \n",
        "- **Bước 2**: Lặp lại bước 1 cho các đường dẫn trong `url_list` (**lưu ý:** kiểm tra các đường dẫn vừa thu được đã nằm trong `url_list` hay không?). Dừng khi đã thu thập được 200 URLs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HFWqw1VOrEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd68d1b-df48-47f5-d12f-d4c692a57b39"
      },
      "source": [
        "import random \n",
        "import urllib.robotparser\n",
        "\n",
        "UAS = (\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\", \n",
        "       \"Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0\",\n",
        "       \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:33.0) Gecko/20100101 Firefox/33.0\",\n",
        "       \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\",\n",
        "       \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\",\n",
        "       \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\",\n",
        "       )\n",
        "\n",
        "ua = UAS[random.randrange(len(UAS))]\n",
        "\n",
        "rp = urllib.robotparser.RobotFileParser()\n",
        "rp.set_url(\"https://en.wikipedia.org/robots.txt\")\n",
        "rp.read()\n",
        "rrate = rp.request_rate(\"*\")\n",
        "rp.crawl_delay(\"*\")\n",
        "\n",
        "head = {\n",
        "  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36',\n",
        "  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "  'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "  'Accept-Encoding': 'none',\n",
        "  'Accept-Language': 'en-US,en;q=0.8',\n",
        "  'Connection': 'keep-alive',\n",
        "  'refere': 'https://example.com',\n",
        "  'cookie': \"\"\"your cookie value ( you can get that from your web page) \"\"\"\n",
        "}\n",
        "\n",
        "def get_urls(url):\n",
        "    try:\n",
        "      r = requests.get(url, headers=head, stream=True, timeout=5)\n",
        "    except requests.exceptions.ConnectionError as err:\n",
        "      print(err)\n",
        "      return None\n",
        "    # TODO\n",
        "    # Lấy các url nằm trong trang web của url này, lưu lại vào biến urls\n",
        "    findall_urls = lambda r : re.findall(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", r.text)\n",
        "    all_urls = findall_urls(r)\n",
        "    cp_all_urls = all_urls.copy()\n",
        "    for url in all_urls:\n",
        "      if (not rp.can_fetch(\"*\", url)):\n",
        "          cp_all_urls.remove(url)\n",
        "    return cp_all_urls\n",
        "\n",
        "def get_urls_recursive(start_url, limit):\n",
        "    urls = [start_url]\n",
        "    for url in urls:\n",
        "        # TODO\n",
        "        # Lấy các url nằm trong trang web của url này, lưu lại vào biến new_urls\n",
        "        new_urls = get_urls(url)\n",
        "        if new_urls is None: \n",
        "          return None\n",
        "        # Với mỗi url mới trong new_urls:\n",
        "        for new_url in new_urls:\n",
        "        #   Nếu nó chưa nằm trong urls thì thêm nó vô  \n",
        "            if new_url not in urls:\n",
        "                urls.append(new_url)\n",
        "        # Nếu kích thước của urls vượt quá limit thì dừng và xóa phần dư thừa\n",
        "        if len(urls) >= limit:\n",
        "            urls=urls[0:limit]\n",
        "            break\n",
        "    return urls\n",
        "url_list = get_urls_recursive('https://en.wikipedia.org/wiki/Web_mining', 20)\n",
        "if url_list is not None:\n",
        "  url_list_cp = url_list.copy()\n",
        "  print('\\n'.join(map(str, enumerate(url_list, start=1))))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'https://en.wikipedia.org/wiki/Web_mining')\n",
            "(2, 'https://creativecommons.org/licenses/by-sa/3.0/')\n",
            "(3, 'https://archive.org/details/webinformationsy00ngua')\n",
            "(4, 'https://archive.org/details/webinformationsy00ngua/page/n33')\n",
            "(5, 'https://archive.org/details/electroniccommer00bauk_698')\n",
            "(6, 'https://archive.org/details/electroniccommer00bauk_698/page/n176')\n",
            "(7, 'https://archive.org/details/webminingapplica0000scim/page/282')\n",
            "(8, 'http://alexandria.tue.nl/repository/freearticles/612259.pdf')\n",
            "(9, 'http://www.cis.unisa.edu.au/')\n",
            "(10, 'https://ieeexplore.ieee.org/document/8765250')\n",
            "(11, 'https://doi.org/10.1109%2FICWR.2019.8765250')\n",
            "(12, 'https://api.semanticscholar.org/CorpusID:198146435')\n",
            "(13, 'https://docs.google.com/open?id=1nU1vrz-gBtSJk3bkb1ls_QuGX2nUPPemECPFlCx0C75MvmQdSqPci6LZWJYf')\n",
            "(14, 'https://ui.adsabs.harvard.edu/abs/2000cs.......11033K')\n",
            "(15, 'https://doi.org/10.1145%2F360402.360406')\n",
            "(16, 'https://api.semanticscholar.org/CorpusID:60455')\n",
            "(17, 'http://library.ifla.org/148/')\n",
            "(18, 'https://doi.org/10.5195%2Fjmla.2019.650')\n",
            "(19, 'http://soave.isti.cnr.it/%7Esilvestr/wp-content/uploads/2007/03/p63-baraglia.pdf')\n",
            "(20, 'http://facweb.cs.depaul.edu/mobasher/classes/ect584/papers/cms-kais.pdf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk4ty3jcRryx"
      },
      "source": [
        "## 2. Thu thập dữ liệu\n",
        "Thu thập dữ liệu từ `url_list`. Lưu trữ dữ liệu thu được vào dictionary data với keys là các từ, values gồm 2 phần tử: \n",
        "- `url_idx_list` với $idx \\in \\left[0,200\\right) \\cap \\mathbb{N}$\n",
        "- `frequency` \n",
        "    \n",
        "Ví dụ: `data['at']=[url_idx_list,frequency]`:\n",
        "- `url_idx_list`: danh sách các url mà trong dữ liệu của chúng (html document) chứa từ \"at\". \n",
        "- `frequency`: tần suất xuất hiện (số lần xuất hiện) của từ `at` trong dữ liệu của **tất cả đường dẫn thu được**.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB5nIZAhQr7-"
      },
      "source": [
        "def text_filter(element):\n",
        "    # TODO\n",
        "    # Cài đặt lại như Lab02\n",
        "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
        "        return False\n",
        "    elif isinstance(element, Comment):\n",
        "        '''Opinion mining?'''\n",
        "        return False\n",
        "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
        "        '''space, return, endline'''\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def wordList(url):\n",
        "    # TODO\n",
        "    # Cài đặt lại như Lab02\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.content, \"html.parser\", from_encoding=\"UTF-8\")\n",
        "    text = soup.findAll(text=True)\n",
        "    filtered_text = list(filter(text_filter, text)) # list của các chuỗi\n",
        "    word_list = []\n",
        "    # TODO:\n",
        "    # Với mỗi chuỗi trong filtered_text:\n",
        "    for text in filtered_text:\n",
        "        #   Thay thế các dấu câu thành khoảng trắng (gợi ý: danh sách các dấu câu: string.punctuation; thay thế: .replace(...))\n",
        "        #   Sử dụng .join() function, nếu một char c nào đó nằm trong string.punctuation thì nó sẽ được thay thế bằng ' '. Ngược lại thì không thay đổi gì\n",
        "        # sau đó join với một string rỗng để có được clean_text\n",
        "        clean_text = ''.join(' ' if c in string.punctuation else c for c in text)\n",
        "        #   Tách chuỗi bởi khoảng trắng (.split(...))\n",
        "        # cắt clean_text vừa có ở trên ra\n",
        "        lst_words_in_clean_text = clean_text.split()\n",
        "        #   Thêm các từ vừa được tách ra vào word_list\n",
        "        word_list = word_list + lst_words_in_clean_text\n",
        "    return word_list\n",
        "\n",
        "def read_url(url, url_idx, data):\n",
        "    # TODO\n",
        "    # Cài đặt lại như Lab02\n",
        "    word_list = wordList(url)\n",
        "    # TODO\n",
        "    # Với mỗi từ w trong word_list:\n",
        "    for i, word in enumerate(word_list):\n",
        "        #   Nếu w chưa có trong data thì khởi tạo data[w] = [[url_idx], 1]\n",
        "        if word not in data.keys():\n",
        "            data[word]=[[url_idx],1]\n",
        "        #   Ngược lại thì thêm url_idx vào data[w][0] (nếu chưa có) và tăng data[w][1] lên 1 đơn vị\n",
        "        else:\n",
        "            if url_idx in data[word][0]:\n",
        "                data[word][0].append(i)\n",
        "            data[word][1]+=1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrPuiiDhQfrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b64bde-4869-493c-d69c-56111c7a0493"
      },
      "source": [
        "data = {}\n",
        "for url_index, url in enumerate(url_list, 1):\n",
        "    print(url)\n",
        "    try:\n",
        "      read_url(url, url_index, data)\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://en.wikipedia.org/wiki/Web_mining\n",
            "https://creativecommons.org/licenses/by-sa/3.0/\n",
            "https://archive.org/details/webinformationsy00ngua\n",
            "https://archive.org/details/webinformationsy00ngua/page/n33\n",
            "https://archive.org/details/electroniccommer00bauk_698\n",
            "https://archive.org/details/electroniccommer00bauk_698/page/n176\n",
            "https://archive.org/details/webminingapplica0000scim/page/282\n",
            "http://alexandria.tue.nl/repository/freearticles/612259.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://www.cis.unisa.edu.au/\n",
            "https://ieeexplore.ieee.org/document/8765250\n",
            "https://doi.org/10.1109%2FICWR.2019.8765250\n",
            "https://api.semanticscholar.org/CorpusID:198146435\n",
            "https://docs.google.com/open?id=1nU1vrz-gBtSJk3bkb1ls_QuGX2nUPPemECPFlCx0C75MvmQdSqPci6LZWJYf\n",
            "https://ui.adsabs.harvard.edu/abs/2000cs.......11033K\n",
            "https://doi.org/10.1145%2F360402.360406\n",
            "https://api.semanticscholar.org/CorpusID:60455\n",
            "http://library.ifla.org/148/\n",
            "https://doi.org/10.5195%2Fjmla.2019.650\n",
            "http://soave.isti.cnr.it/%7Esilvestr/wp-content/uploads/2007/03/p63-baraglia.pdf\n",
            "http://facweb.cs.depaul.edu/mobasher/classes/ect584/papers/cms-kais.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "740eXy7pRryx"
      },
      "source": [
        "## 3. Tiền xử lý\n",
        "Loại bỏ các item trong data mà key là các stopword.\n",
        "\n",
        "**Ngữ liệu:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC58K3Q3Rryy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d57bfd-bd42-4300-ba1f-30bb2516545e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "english_stopwords = stopwords.words('english')\n",
        "print(english_stopwords)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cylo7trpRnun"
      },
      "source": [
        "# TODO\n",
        "# Loại bỏ các key của biến data mà nằm trong danh sách english_stopwords\n",
        "data_copy = data.copy()\n",
        "for key in data_copy:\n",
        "  if key in english_stopwords:\n",
        "    data.pop(key)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEoxNVHqRry2"
      },
      "source": [
        "## 4. Lưu trữ và biểu diễn dữ liệu\n",
        "Sử dụng pickle lưu lại data với tên file raw_data.\n",
        "### 4.1 Cơ sở dữ liệu giao tác:\n",
        "Thông thường, các cơ sở dữ liệu giao tác được lưu trong flat files (các tập phẳng) thay vì trong một hệ cơ sở dữ liệu. Các item là các số nguyên không âm, mỗi giao tác tương ứng với một dòng các số nguyên phân tách nhau bằng khoảng trắng.\n",
        "Ví dụ:\n",
        "\n",
        "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
        "\n",
        "30 31 32 \n",
        "\n",
        "33 34 35 \n",
        "\n",
        "36 37 38 39 40 41 42 43 44 45 46 \n",
        "\n",
        "38 39 47 48 \n",
        "\n",
        "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
        "\n",
        "32 41 59 60 61 62 \n",
        "\n",
        "3 39 48 \n",
        "\n",
        "63 64 65 66 67 68 \n",
        "\n",
        "32 69 \n",
        "\n",
        "48 70 71 72 \n",
        "\n",
        "39 73 74 75 76 77 78 79 \n",
        "\n",
        "36 38 39 41 48 79 80 81 \n",
        "\n",
        "82 83 84 \n",
        "\n",
        "41 85 86 87 88 \n",
        "\n",
        "39 48 89 90 91 92 93 94 95 96 97 98 99 100 101 \n",
        "\n",
        "36 38 39 48 89 \n",
        "\n",
        "39 41 102 103 104 105 106 107 108 \n",
        "\n",
        "38 39 41 109 110 \n",
        "\n",
        "39 111 112 113 114 115 116 117 118 \n",
        "\n",
        "119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 \n",
        "\n",
        "48 134 135 136 \n",
        "\n",
        "39 48 137 138 139 140 141 142 143 144 145 146 147 148 149 \n",
        "\n",
        "39 150 151 152 \n",
        "\n",
        "38 39 56 153 154 155 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l--TVLEoN7R"
      },
      "source": [
        "with open('raw_data', 'wb') as f:\n",
        "    # TODO\n",
        "    pickle.dump(data,f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = {}\n",
        "with open(\"raw_data\", \"rb\") as f:\n",
        "    output = pickle.load(f)\n",
        "\n",
        "from itertools import islice\n",
        "list(islice(output.items(), 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuISb-CNZ3EX",
        "outputId": "a4816939-f99e-452e-d29b-4223dcee0abc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Web',\n",
              "  [[1,\n",
              "    56,\n",
              "    88,\n",
              "    92,\n",
              "    102,\n",
              "    106,\n",
              "    111,\n",
              "    133,\n",
              "    136,\n",
              "    146,\n",
              "    149,\n",
              "    152,\n",
              "    162,\n",
              "    170,\n",
              "    173,\n",
              "    176,\n",
              "    192,\n",
              "    195,\n",
              "    210,\n",
              "    219,\n",
              "    221,\n",
              "    237,\n",
              "    251,\n",
              "    326,\n",
              "    520,\n",
              "    541,\n",
              "    545,\n",
              "    560,\n",
              "    563,\n",
              "    579,\n",
              "    639,\n",
              "    701,\n",
              "    718,\n",
              "    774,\n",
              "    795,\n",
              "    801,\n",
              "    818,\n",
              "    844,\n",
              "    886,\n",
              "    903],\n",
              "   139]),\n",
              " ('mining',\n",
              "  [[1,\n",
              "    57,\n",
              "    85,\n",
              "    89,\n",
              "    94,\n",
              "    104,\n",
              "    108,\n",
              "    113,\n",
              "    134,\n",
              "    137,\n",
              "    148,\n",
              "    151,\n",
              "    154,\n",
              "    163,\n",
              "    168,\n",
              "    172,\n",
              "    175,\n",
              "    178,\n",
              "    194,\n",
              "    197,\n",
              "    223,\n",
              "    253,\n",
              "    293,\n",
              "    311,\n",
              "    328,\n",
              "    461,\n",
              "    495,\n",
              "    522,\n",
              "    543,\n",
              "    547,\n",
              "    557,\n",
              "    562,\n",
              "    565,\n",
              "    568,\n",
              "    629,\n",
              "    641,\n",
              "    702,\n",
              "    720,\n",
              "    747],\n",
              "   116]),\n",
              " ('From', [[1], 4]),\n",
              " ('Wikipedia', [[1, 971], 3]),\n",
              " ('free', [[1], 7]),\n",
              " ('encyclopedia', [[1], 1]),\n",
              " ('‹', [[1], 1]),\n",
              " ('The', [[1, 36, 40, 155, 289, 397, 439, 582, 684, 725], 50]),\n",
              " ('considered', [[1, 236, 343, 454], 4]),\n",
              " ('merging', [[1], 1])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvUgw0VkRry3"
      },
      "source": [
        "### 4.2 Xuất dataset\n",
        "Lưu một cơ sở dữ liệu giao tác (transactional database) vào file t_data.txt: \n",
        "- Các item tương ứng với url_idx\n",
        "- Mỗi transaction tương ứng với một từ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZorK46LdSCfi"
      },
      "source": [
        "with open('t_data.txt', 'w') as f:\n",
        "    for word, (url_list, freq) in data.items():\n",
        "        print(*url_list, file=f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OXVFoHiRry3"
      },
      "source": [
        "## 5. Truy vấn and, or, not\n",
        "Ví dụ: \n",
        "- Truy vấn `and` câu `web mining`: trả về đường dẫn tới các trang web có cả 2 từ web và từ mining. \n",
        "- Truy vấn `or` câu `web mining`: trả về đường dẫn tới các trang web có từ web hoặc từ mining.\n",
        "- Truy vấn `not` câu `web mining`: trả về đường dẫn tới các trang không có cả từ web và từ mining.\n",
        "\n",
        "*GỢI Ý: TÁCH CÂU TRUY VẤN THÀNH CÁC TỪ TƯƠNG TỰ PHƯƠNG PHÁP LÀM Ở LAB02.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keRglQuZwBBm",
        "outputId": "23d2e031-44db-49b6-c2a2-a35d65380b77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20, 55534]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sybM0yLHRry6"
      },
      "source": [
        "ret=[]\n",
        "def andRetrieval(ret, sentence):\n",
        "    '''Parameters\n",
        "    -----------------------\n",
        "    ret: url_list\n",
        "    sentence: query'''\n",
        "    # TODO\n",
        "    ### split sentence (separator ' ') into terms\n",
        "    terms = sentence.split(\" \")\n",
        "    ### find urls have all terms: urls\n",
        "    flag = 0\n",
        "    for url in url_list_cp:\n",
        "      for term in terms:\n",
        "        if (url.find(term) == -1):\n",
        "          flag = 0\n",
        "          break\n",
        "        else:\n",
        "          flag = 1\n",
        "          continue\n",
        "      if (flag):\n",
        "        ret.append(url)\n",
        "    ### if len(ret)==0:  return urls\n",
        "    if len(ret)==0:\n",
        "      return ret\n",
        "    ### else update ret with urls: intersection of ret and urls \n",
        "    else:\n",
        "      return ret\n",
        "\n",
        "\n",
        "def orRetrieval(ret, sentence):\n",
        "    '''Parameters\n",
        "    -----------------------\n",
        "    ret: url_list\n",
        "    sentence: query'''\n",
        "    # TODO\n",
        "    ### split sentence (separator ' ') into terms\n",
        "    terms=sentence.split(\" \")\n",
        "    ### find urls have all terms: urls\n",
        "    for url in url_list_cp:\n",
        "      for term in terms:\n",
        "    ### find urls have at least 1 term: urls\n",
        "        if (url.find(term)):\n",
        "          ret.append(url)\n",
        "          break\n",
        "    ### update ret with urls: extend ret with urls\n",
        "    return ret\n",
        "\n",
        "def notRetrieval(ret, sentence):\n",
        "    '''Parameters\n",
        "    -----------------------\n",
        "    ret: url_list\n",
        "    sentence: query'''\n",
        "    # TODO\n",
        "    ### split sentence (separator ' ') into terms\n",
        "    terms=sentence.split(\" \")\n",
        "    ### find urls have at least 1 term: urls\n",
        "    urls = []\n",
        "    flag = 0\n",
        "    for url in url_list_cp:\n",
        "      for term in terms:\n",
        "        if (url.find(term)):\n",
        "          flag = 0\n",
        "          break\n",
        "        else:\n",
        "          flag = 1\n",
        "      if flag==1:\n",
        "        urls.append(url)\n",
        "    ### update ret with urls: remove urls from ret \n",
        "    ret = list(set(ret) - set(urls))\n",
        "    return ret"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkp7ej3qxRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bd523d-e71e-41ff-a770-46f4a1c05e63"
      },
      "source": [
        "print(andRetrieval([], 'web mining'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://archive.org/details/webminingapplica0000scim/page/282']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(orRetrieval([], 'web mining'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4WiPtQm0QhN",
        "outputId": "a458adb4-2513-46e4-8250-d4a58a9530d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://en.wikipedia.org/wiki/Web_mining', 'https://creativecommons.org/licenses/by-sa/3.0/', 'https://archive.org/details/webinformationsy00ngua', 'https://archive.org/details/webinformationsy00ngua/page/n33', 'https://archive.org/details/electroniccommer00bauk_698', 'https://archive.org/details/electroniccommer00bauk_698/page/n176', 'https://archive.org/details/webminingapplica0000scim/page/282', 'http://alexandria.tue.nl/repository/freearticles/612259.pdf', 'http://www.cis.unisa.edu.au/', 'https://ieeexplore.ieee.org/document/8765250', 'https://doi.org/10.1109%2FICWR.2019.8765250', 'https://api.semanticscholar.org/CorpusID:198146435', 'https://docs.google.com/open?id=1nU1vrz-gBtSJk3bkb1ls_QuGX2nUPPemECPFlCx0C75MvmQdSqPci6LZWJYf', 'https://ui.adsabs.harvard.edu/abs/2000cs.......11033K', 'https://doi.org/10.1145%2F360402.360406', 'https://api.semanticscholar.org/CorpusID:60455', 'http://library.ifla.org/148/', 'https://doi.org/10.5195%2Fjmla.2019.650', 'http://soave.isti.cnr.it/%7Esilvestr/wp-content/uploads/2007/03/p63-baraglia.pdf', 'http://facweb.cs.depaul.edu/mobasher/classes/ect584/papers/cms-kais.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(notRetrieval(url_list_cp, 'web mining'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIDjl7Qn0PlX",
        "outputId": "63869a5f-7829-4452-f4ab-7bbede62b4b8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['http://facweb.cs.depaul.edu/mobasher/classes/ect584/papers/cms-kais.pdf', 'https://en.wikipedia.org/wiki/Web_mining', 'https://archive.org/details/webminingapplica0000scim/page/282', 'https://archive.org/details/webinformationsy00ngua/page/n33', 'https://archive.org/details/webinformationsy00ngua']\n"
          ]
        }
      ]
    }
  ]
}